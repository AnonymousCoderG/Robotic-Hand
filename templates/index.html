<!DOCTYPE html>
<html>
<head>
    <title>Robotic Hand Control (Async)</title>
    <meta charset="utf-8">
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js" crossorigin="anonymous"></script>
    <style>
        body { font-family: sans-serif; display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100vh; margin: 0; background-color: #2c3e50; color: white; }
        #output_canvas { border: 5px solid #3498db; border-radius: 10px; background-color: #000; transform: scaleX(-1); } /* Flip the canvas for a mirror effect */
        #webcam { display: none; }
        #status { margin-top: 15px; font-size: 1.2em; color: #ecf0f1; }
    </style>
</head>
<body>
    <h1>Robotic Hand Control Stream</h1>
    <canvas id="output_canvas" width="640px" height="480px"></canvas>
    <video id="webcam" autoplay playsinline style="display:none;"></video>
    <p id="status">Status: Initializing...</p>

    <script type="module">
        const videoElement = document.getElementById('webcam');
        const canvasElement = document.getElementById('output_canvas');
        const canvasCtx = canvasElement.getContext('2d');
        const statusElement = document.getElementById('status');

        const tipIds = [4, 8, 12, 16, 20];
        let lastSendTime = 0;
        const SEND_INTERVAL = 250; // Milliseconds (sends data a max of 4 times per second)

        // Create the Web Worker to run ML in the background
        const worker = new Worker('/static/worker.js');

        // This function handles fully processed results FROM the worker
        worker.onmessage = (event) => {
            const { landmarks, image } = event.data;

            // The main thread's ONLY job is to draw the result. This is very fast.
            canvasCtx.save();
            canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
            canvasCtx.drawImage(image, 0, 0, canvasElement.width, canvasElement.height);

            if (landmarks && landmarks.length > 0) {
                const handLandmarks = landmarks[0];
                drawConnectors(canvasCtx, handLandmarks, HANDS_CONNECTIONS, { color: '#FFFFFF', lineWidth: 5 });
                drawLandmarks(canvasCtx, handLandmarks, { color: '#FF0000', lineWidth: 2 });
                
                const currentTime = Date.now();
                if (currentTime - lastSendTime > SEND_INTERVAL) {
                    let fingers = [];
                    // The thumb logic is now correct for the flipped canvas
                    if (handLandmarks[tipIds[0]].x > handLandmarks[tipIds[0] - 1].x) { fingers.push(1); } else { fingers.push(0); }
                    for (let id = 1; id < 5; id++) {
                        if (handLandmarks[tipIds[id]].y < handLandmarks[tipIds[id] - 2].y) { fingers.push(1); } else { fingers.push(0); }
                    }
                    
                    const angles = { "thumb": fingers[0]===1?90:0, "index": fingers[1]===1?0:90, "middle": fingers[2]===1?0:180, "ring": fingers[3]===1?0:90, "pinky": fingers[4]===1?90:0 };

                    fetch('/api/hand_data', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(angles) })
                        .then(() => statusElement.textContent = `Status: Command Sent!`);
                    lastSendTime = currentTime;
                }
            } else {
                 statusElement.textContent = "Status: No hand detected.";
            }
            canvasCtx.restore();
        };

        // This function sends video frames TO the worker in a continuous, non-blocking loop
        async function predictWebcam() {
            // Create an ImageBitmap from the current video frame
            const imageBitmap = await createImageBitmap(videoElement);
            
            // Post the ImageBitmap to the worker for background processing.
            worker.postMessage(imageBitmap, [imageBitmap]);

            // Call this function again on the next available screen repaint
            requestAnimationFrame(predictWebcam);
        }

        // Start the camera and, once it's playing, start the processing loop
        async function startCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 640, height: 480 } });
                videoElement.srcObject = stream;
                videoElement.addEventListener('playing', predictWebcam);
                statusElement.textContent = "Status: Camera started. Show your hand.";
            } catch (err) {
                console.error("Error accessing camera: ", err);
                statusElement.textContent = "Error: Could not access camera.";
            }
        }
        
        startCamera();
    </script>
</body>
</html>